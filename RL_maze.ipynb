{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwmBwmBQKxGFWCJxWzGnTI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bhavya05shah/RL_maze/blob/main/RL_maze.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "z7ckvVxcaS9i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Maze:\n",
        "    def __init__(self, maze, start_position, goal_position):\n",
        "        self.maze = maze\n",
        "        self.maze_height = maze_layout.shape[0] # Get the height of the maze (number of rows)\n",
        "        self.maze_width = maze_layout.shape[1]  # Get the width of the maze (number of columns)\n",
        "        self.start_position = start_position    # Set the start position in the maze as a tuple (x, y)\n",
        "        self.goal_position = goal_position      # Set the goal position in the maze as a tuple (x, y)\n",
        "\n",
        "    def show_maze(self):\n",
        "        # Visualizing the maze\n",
        "        plt.figure(figsize=(5,5))\n",
        "        plt.imshow(self.maze, cmap='gray')\n",
        "\n",
        "        # Adding start and goal positions to the maze\n",
        "        plt.text(self.start_position[0], self.start_position[1], 'S', ha='center', va='center', color='red', fontsize=20)\n",
        "        plt.text(self.goal_position[0], self.goal_position[1], 'G', ha='center', va='center', color='green', fontsize=20)\n",
        "\n",
        "        # Removing ticks and labels\n",
        "        plt.xticks([]), plt.yticks([])\n",
        "        plt.show()\n"
      ],
      "metadata": {
        "id": "Xu5xS1afU8sK"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create any maze layout you'd like, here's an example\n",
        "maze_layout = np.array([\n",
        "    [0, 1, 0, 0, 0],\n",
        "    [0, 1, 1, 1, 0],\n",
        "    [0, 0, 0, 1, 0],\n",
        "    [1, 1, 0, 1, 1],\n",
        "    [0, 0, 0, 0, 0]\n",
        "])\n",
        "\n",
        "# Create an instance of the maze and set the starting and ending positions\n",
        "maze = Maze(maze_layout, (0, 0), (4, 4))\n",
        "# Visualize the maze\n",
        "maze.show_maze()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 422
        },
        "id": "-KyZLN8LXHyf",
        "outputId": "b3fc8570-32b1-457c-8354-02d7ee34a2bd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "actions = [(-1, 0), # Up: Moving one step up, reducing the row index by 1\n",
        "          (1, 0),   # Down: Moving on step down, increasing the row index by 1\n",
        "          (0, -1),  # Left: Moving one step to the left, reducing the column index by 1\n",
        "          (0, 1)]   # Right: Moving one step to the right, increasing the column index by 1\n",
        "\n",
        "class QLearningAgent:\n",
        "    def __init__(self, maze, learning_rate=0.1, discount_factor=0.9, exploration_start=1.0, exploration_end=0.01, num_episodes=100):\n",
        "        # Initialize a Q-table containing all zeros\n",
        "        self.q_table = np.zeros((maze.maze_height, maze.maze_width, 4)) # 4 actions: Up, Down, Left, Right\n",
        "        self.learning_rate = learning_rate          # Learning rate controls how much the agent updates its Q-values after each action\n",
        "        self.discount_factor = discount_factor      # Discount factor determines the importance of future rewards in the agent's decisions\n",
        "        self.exploration_start = exploration_start  # Exploration rate determines the likelihood of the agent taking a random action\n",
        "        self.exploration_end = exploration_end\n",
        "        self.num_episodes = num_episodes\n",
        "\n",
        "    def get_exploration_rate(self, current_episode):\n",
        "        # Calculating the current exploration rate\n",
        "        exploration_rate = self.exploration_start * (self.exploration_end / self.exploration_start) ** (current_episode / self.num_episodes)\n",
        "        return exploration_rate\n",
        "\n",
        "    def get_action(self, state, current_episode):\n",
        "        exploration_rate = self.get_exploration_rate(current_episode)\n",
        "        # Select the sate randomly (exploration) or using the Q-table (exploitation)\n",
        "        if np.random.rand() < exploration_rate:\n",
        "            return np.random.randint(4) # Choose a random action\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state]) # State with highest Q value\n",
        "\n",
        "    def update_q_table(self, state, action, next_state, reward):\n",
        "        best_next_action = np.argmax(self.q_table[next_state])\n",
        "        current_q_value = self.q_table[state][action]\n",
        "\n",
        "        # Q-value updated\n",
        "        new_q_value = current_q_value + self.learning_rate * (reward + self.discount_factor * self.q_table[next_state][best_next_action] - current_q_value)\n",
        "        self.q_table[state][action] = new_q_value\n"
      ],
      "metadata": {
        "id": "eweypSegXKC9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here the reward system has been defined by its reward,penalty(which is of two types)\n",
        "goal_reward = 100\n",
        "wall_penalty = -10\n",
        "step_penalty = -1"
      ],
      "metadata": {
        "id": "Fm4deXqTXdR1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#For a single episode.\n",
        "\n",
        "def finish_episode(agent, maze, current_episode, train=True):\n",
        "    # Initialize the agent's current state to the maze's start position\n",
        "    current_state = maze.start_position\n",
        "    is_done = False\n",
        "    episode_reward = 0\n",
        "    episode_step = 0\n",
        "    path = [current_state]\n",
        "\n",
        "    # Loop of episodes\n",
        "    while not is_done:\n",
        "        action = agent.get_action(current_state, current_episode)\n",
        "        next_state = (current_state[0] + actions[action][0], current_state[1] + actions[action][1])\n",
        "\n",
        "        # Checking if the next state is out of bounds or hitting a wall\n",
        "        if next_state[0] < 0 or next_state[0] >= maze.maze_height or next_state[1] < 0 or next_state[1] >= maze.maze_width or maze.maze[next_state[1]][next_state[0]] == 1:\n",
        "            reward = wall_penalty\n",
        "            next_state = current_state\n",
        "        # Checking if the agent has reached its goal:\n",
        "        elif next_state == (maze.goal_position):\n",
        "            path.append(current_state)\n",
        "            reward = goal_reward\n",
        "            is_done = True\n",
        "        # The agent takes a step but hasn't reached the goal yet\n",
        "        else:\n",
        "            path.append(current_state)\n",
        "            reward = step_penalty\n",
        "\n",
        "        # Update the cumulative reward and step count for that episode\n",
        "        episode_reward += reward\n",
        "        episode_step += 1\n",
        "\n",
        "        # Update the agent's Q-table\n",
        "        if train == True:\n",
        "            agent.update_q_table(current_state, action, next_state, reward)\n",
        "        current_state = next_state\n",
        "\n",
        "    return episode_reward, episode_step, path\n"
      ],
      "metadata": {
        "id": "SpQSjb4LXhw9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XVlceKJqXo7f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}